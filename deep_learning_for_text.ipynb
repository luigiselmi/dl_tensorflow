{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiselmi/dl_tensorflow/blob/main/deep_learning_for_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f667f06-5a62-4ce6-bc78-f9fd0d96175a",
      "metadata": {
        "id": "6f667f06-5a62-4ce6-bc78-f9fd0d96175a"
      },
      "source": [
        "# Deep learning for text\n",
        "In this notebook we will use deep learning algorithms to perform some tasks on text. Common tasks are: text classification, content filtering, sentiment analysis, translation, text summarization, language modeling. Since deep learning models are differentiable functions that can only process tensors of numbers we have to transform text into numerical tensors. The steps to transform text into numbers are\n",
        "\n",
        "1. Text standardization\n",
        "2. Tokenization\n",
        "3. Convert the tokens into numerical arrays\n",
        "\n",
        "In the first step we perform the same kind of transformations used to build a search engine: lower case, remove punctuation, word stemming. The words left represent the tokens, elements of a \"clean\" vocabulary. We can also build token made of two words, called bigram, or more words called N-gram. The number of tokens N defines the dimensionality of a space where each token represents a dimension and a text can be represented as vector in such space. We can define a metric is such a space so that we can measure the distance between two sequences of tokens. Depending on the task at hand, we might need to process the tokens in the order in which they appear in the text. In this case we will build a sequence model. If our task doesn't need the order of the tokens we will build a bag-of-words model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540a8b78-6869-4a0f-82c6-b01e808cc1bc",
      "metadata": {
        "id": "540a8b78-6869-4a0f-82c6-b01e808cc1bc"
      },
      "source": [
        "## Bag-of-words models\n",
        "We will use the [IMDB dataset]('https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) to train our bag-of-words model for sentiment analysis, that is a kind of text classification. The dataset contains 50k movie reviews. We download the dataset and extract the files into a folder. The dataset contains two subfolders train/ and test/ each containing 25k reviews split into two subfolders pos/ and neg/ with 12500 txt files. Each file contains a short text, the content of the review. The name of the file is created from the review's unique identifier and the score given to the movie. A score equal or higher than 7 is positive, a score equal or lower than 4 is negative. This same example is available for PyTorch in [another repository](https://github.com/luigiselmi/machine_learning_notes/blob/main/pml3/sentiment_analysis.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12d88618-a75c-405a-bb56-fae698e09738",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12d88618-a75c-405a-bb56-fae698e09738",
        "outputId": "3473d672-092a-43d6-e3c6-c2514150af2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-27 14:04:54 URL:https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz [84125825/84125825] -> \"./data/aclImdb_v1.tar.gz\" [1]\n"
          ]
        }
      ],
      "source": [
        "!wget -nv 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' -P './data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d94aab15-0e4c-4ef9-beab-f2eb529ce992",
      "metadata": {
        "id": "d94aab15-0e4c-4ef9-beab-f2eb529ce992"
      },
      "outputs": [],
      "source": [
        "!tar -xf data/aclImdb_v1.tar.gz -C data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf722b0b-3366-4fbc-971d-33636d4c78fc",
      "metadata": {
        "id": "cf722b0b-3366-4fbc-971d-33636d4c78fc"
      },
      "outputs": [],
      "source": [
        "!rm -r data/aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d3c2491-d13c-4849-b566-f55dc207ad1d",
      "metadata": {
        "id": "2d3c2491-d13c-4849-b566-f55dc207ad1d"
      },
      "source": [
        "We print the content of one review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9d10772-828c-4207-b2da-52dd2a6aea69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9d10772-828c-4207-b2da-52dd2a6aea69",
        "outputId": "4f2f6f16-8351-4097-d158-1084ba6aa41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!cat data/aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb335163-cabd-43bb-ad2e-9e498c800021",
      "metadata": {
        "id": "fb335163-cabd-43bb-ad2e-9e498c800021"
      },
      "source": [
        "We use 20% of the training data for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3182c20-454d-4aad-a0c1-bd5cb003c8bd",
      "metadata": {
        "id": "d3182c20-454d-4aad-a0c1-bd5cb003c8bd"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "base_dir = pathlib.Path(\"data/aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5559307-9bd1-41d6-81f5-43d5a2880567",
      "metadata": {
        "id": "f5559307-9bd1-41d6-81f5-43d5a2880567"
      },
      "source": [
        "We create three datasets for train, validation and test. Each dataset contains a number of batches with 32 text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ee5b14c8-cf51-48f6-88c8-48545f281f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee5b14c8-cf51-48f6-88c8-48545f281f0d",
        "outputId": "6731e51e-85f4-45ce-a393-2865b413695d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory('data/aclImdb/train', batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory('data/aclImdb/val', batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory('data/aclImdb/test', batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d4a0fd7-61f0-49c4-867a-c1198358c6b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d4a0fd7-61f0-49c4-867a-c1198358c6b5",
        "outputId": "83f1278c-17b2-495a-8533-b75c73dfaec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches for training: 625\n",
            "Number of batches for validation: 157\n",
            "Number of batches for test: 782\n"
          ]
        }
      ],
      "source": [
        "print('Number of batches for training: {:d}'.format(len(train_ds)))\n",
        "print('Number of batches for validation: {:d}'.format(len(val_ds)))\n",
        "print('Number of batches for test: {:d}'.format(len(test_ds)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-SSBbhpTMrk"
      },
      "id": "4-SSBbhpTMrk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}