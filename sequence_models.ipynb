{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a64100d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/luigiselmi/dl_tensorflow/blob/main/sequence_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418208bb-73f0-4b89-8d51-7bdc2f07d9f5",
   "metadata": {
    "id": "418208bb-73f0-4b89-8d51-7bdc2f07d9f5"
   },
   "source": [
    "# Deep learning for text: sequence models\n",
    "With the improvement using the bigram data we have seen that the order of words matters. Now we want to study more in depth how to exploit the order of words in texts to perform several tasks that are common when using textual data. Instead of creating N-grams we will let the model figure out how to use a text to address the task at hand using a sequence model. We start by assigning an integer index to each word in a vocabulary that will be mapped to a vector by computing the word embedding. One other approach was to map a word to a vector using the one-hot encoding technique. The problem with this technique is that the dimensionality of the vector space will be the same as the cardinality (size) of the vocabulary. The dimensionality of the embeddings can be much lower. These words embeddigs represent a base vectors that will be used to represent every text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wPDj_qrLunvn",
   "metadata": {
    "id": "wPDj_qrLunvn"
   },
   "source": [
    "We replace the Tensorflow version available in Colab to install one that is closer to the version 2.6 used in the book by Chollet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mzVQBjtUhYWs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzVQBjtUhYWs",
    "outputId": "1f91ef77-ec60-420a-faed-b4a9c6c9fa0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.8\n",
      "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (24.3.25)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.11.0)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.16.0)\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8)\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8)\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8)\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8) (1.64.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.27.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.32.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.2)\n",
      "Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.1\n",
      "    Uninstalling google-auth-oauthlib-1.2.1:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.23.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "l0HGkWLth21M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0HGkWLth21M",
    "outputId": "0afb1c38-8b19-4905-d630-aae7f3b6eff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.0\n",
      "Keras version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "print('Keras version: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0HW8cyyvtJEt",
   "metadata": {
    "id": "0HW8cyyvtJEt"
   },
   "source": [
    "## The IMDB dataset\n",
    "We download the same IMDB dataset used in the bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2XOsQDm8Zy6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XOsQDm8Zy6a",
    "outputId": "f7a5a05d-a077-40fe-8550-d0ff565ee18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 08:43:11 URL:https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz [84125825/84125825] -> \"./data/aclImdb_v1.tar.gz\" [1]\n"
     ]
    }
   ],
   "source": [
    "!wget -nv 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz' -P './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sTzYpcX0Z49d",
   "metadata": {
    "id": "sTzYpcX0Z49d"
   },
   "outputs": [],
   "source": [
    "!tar -xf data/aclImdb_v1.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2w0NmkUwaAkP",
   "metadata": {
    "id": "2w0NmkUwaAkP"
   },
   "outputs": [],
   "source": [
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hG1kr2LAxV8J",
   "metadata": {
    "id": "hG1kr2LAxV8J"
   },
   "source": [
    "The dataset is divided into a train and a test set. We take 20% of the reviews from the train set to create a validation set tha twill be used to monitor the performance of the model during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8vHmsESAaNHZ",
   "metadata": {
    "id": "8vHmsESAaNHZ"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "base_dir = pathlib.Path(\"data/aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nwzqKkMGx6eO",
   "metadata": {
    "id": "nwzqKkMGx6eO"
   },
   "source": [
    "We instantiate three Keras Dataset to handle the datasets using an [utility function](https://keras.io/api/data_loading/text/) from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "LcyejnQRbRjg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcyejnQRbRjg",
    "outputId": "b7ae56f3-bc3f-4108-c311-6bec0c0b55f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory('data/aclImdb/train', batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory('data/aclImdb/val', batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory('data/aclImdb/test', batch_size=batch_size)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qOEUsk1XzGHK",
   "metadata": {
    "id": "qOEUsk1XzGHK"
   },
   "source": [
    "We use only the first 600 words in the reviews and we set the vocabulary size to 20000 so each word will be mapped to an integer and a review will be represented as a vector of maximum 20000 integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37812cb-1e63-4f4e-8c9b-30c0f66bee65",
   "metadata": {
    "id": "d37812cb-1e63-4f4e-8c9b-30c0f66bee65"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jreki8_m2eK0",
   "metadata": {
    "id": "jreki8_m2eK0"
   },
   "source": [
    "We prepare the dataset where each review example is an integer vector representation of its text and the label, 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8046dea-633e-436c-af1a-f3a7d4625f80",
   "metadata": {
    "id": "a8046dea-633e-436c-af1a-f3a7d4625f80"
   },
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0iu8t_h21ps",
   "metadata": {
    "id": "v0iu8t_h21ps"
   },
   "source": [
    "## The sequence model\n",
    "We can now create a first sequence model. One step is the computation of the words embeddings from the integer vector that we have used so far to represent a review. We set the dimensionality of the embeddings to 256. The number of parameters for the embeddings is the product of the integer vector dimensions, that is the size of the vocabulary, times the dimensionality of the embeddings. In other words, the shape of the embedding matrix is (vocabulary size, embedding dimensionality), in our case (20000, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hehq5ItrjSLv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hehq5ItrjSLv",
    "outputId": "a2d0cc0e-9239-4647-f55f-81cea87f97ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               73984     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kjvrsv3lpW4S",
   "metadata": {
    "id": "Kjvrsv3lpW4S"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Z7_Hyawpaee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Z7_Hyawpaee",
    "outputId": "b1ec0a43-4396-47ec-c80f-e18b85e89c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 459s 730ms/step - loss: 0.4689 - accuracy: 0.7896 - val_loss: 0.4226 - val_accuracy: 0.8120\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 453s 724ms/step - loss: 0.3011 - accuracy: 0.8901 - val_loss: 0.2993 - val_accuracy: 0.8838\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 458s 732ms/step - loss: 0.2538 - accuracy: 0.9135 - val_loss: 0.3076 - val_accuracy: 0.8842\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 451s 722ms/step - loss: 0.2053 - accuracy: 0.9298 - val_loss: 0.3093 - val_accuracy: 0.8810\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 447s 716ms/step - loss: 0.1714 - accuracy: 0.9427 - val_loss: 0.3492 - val_accuracy: 0.8848\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 454s 727ms/step - loss: 0.1501 - accuracy: 0.9517 - val_loss: 0.3239 - val_accuracy: 0.8854\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 446s 713ms/step - loss: 0.1258 - accuracy: 0.9603 - val_loss: 0.3454 - val_accuracy: 0.8758\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 458s 732ms/step - loss: 0.1040 - accuracy: 0.9670 - val_loss: 0.3476 - val_accuracy: 0.8744\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 455s 728ms/step - loss: 0.0846 - accuracy: 0.9732 - val_loss: 0.3863 - val_accuracy: 0.8744\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 452s 723ms/step - loss: 0.0692 - accuracy: 0.9803 - val_loss: 0.4413 - val_accuracy: 0.8706\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.3371 - accuracy: 0.8709\n",
      "Test acc: 0.871\n"
     ]
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lSyhhIFTF7Ec",
   "metadata": {
    "id": "lSyhhIFTF7Ec"
   },
   "source": [
    "The accuracy of the model is still somewhat lower than the bag-of-words model and the bigrams. One reason is that we use a subset of each review of 600 words. Another reason is that reviews shorter than 600 words are padded with zeros. We could reduce the second problem by setting the mask_zero argument of the Embedding layer to True so that the zeros will be masked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rrQn9-tE_bAO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrQn9-tE_bAO",
    "outputId": "67e6d23f-fec2-4dd3-c335-b83f3e80204d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 101s 128ms/step - loss: 0.3371 - accuracy: 0.8709\n",
      "Test acc: 0.871\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cHt5SG-rLNTN",
   "metadata": {
    "id": "cHt5SG-rLNTN"
   },
   "source": [
    "## Pretrained word embeddings\n",
    "We have seen that in our case learning the embeddings requires learning more than 5 millions parameters. We can avoid this step by using a pretrained embeddings that has a semantics, that is concepts and relationships between them, similar to that in our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "krrFms-d_dPt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krrFms-d_dPt",
    "outputId": "b1524b8b-a93f-485a-ca00-fac80fa87e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 08:47:55 URL:https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [862182613/862182613] -> \"./data/glove.6B.zip\" [1]\n"
     ]
    }
   ],
   "source": [
    "!wget -nv 'http://nlp.stanford.edu/data/glove.6B.zip'  -P './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qm-Amd1sMZYh",
   "metadata": {
    "id": "qm-Amd1sMZYh"
   },
   "outputs": [],
   "source": [
    "!unzip -q data/glove.6B.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fXP4E0jwRDQr",
   "metadata": {
    "id": "fXP4E0jwRDQr"
   },
   "source": [
    "We count the number of word embeddings in the pretrained dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "as6qbJSTN4at",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as6qbJSTN4at",
    "outputId": "dad741eb-b029-42c1-c825-fefd26830769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path_to_glove_file = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "KQd1E0yqOjV6",
   "metadata": {
    "id": "KQd1E0yqOjV6"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "  if i < max_tokens:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6sJzGTL5Otnh",
   "metadata": {
    "id": "6sJzGTL5Otnh"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "  max_tokens,\n",
    "  embedding_dim,\n",
    "  embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "  trainable=False,\n",
    "  mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDmz2zdJScCt",
   "metadata": {
    "id": "jDmz2zdJScCt"
   },
   "source": [
    "We define a model with the pretrained embeddings so that the network does not need to learn the embedding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mF5dF0J8R4Qa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mF5dF0J8R4Qa",
    "outputId": "69d63a71-2cf3-4dcd-cd90-b2ad5c609efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               34048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,034,113\n",
      "Trainable params: 34,113\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "  keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "  save_best_only=True)\n",
    "]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "EQcUNSgfSQEb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQcUNSgfSQEb",
    "outputId": "ecaac96f-0511-4cb1-fbe3-e4cf121ed3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 565s 887ms/step - loss: 0.5720 - accuracy: 0.6983 - val_loss: 0.4350 - val_accuracy: 0.8048\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 548s 876ms/step - loss: 0.4509 - accuracy: 0.7971 - val_loss: 0.4009 - val_accuracy: 0.8184\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 545s 872ms/step - loss: 0.4052 - accuracy: 0.8184 - val_loss: 0.3922 - val_accuracy: 0.8218\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 555s 888ms/step - loss: 0.3733 - accuracy: 0.8376 - val_loss: 0.3911 - val_accuracy: 0.8238\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 556s 890ms/step - loss: 0.3444 - accuracy: 0.8531 - val_loss: 0.3453 - val_accuracy: 0.8526\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 552s 882ms/step - loss: 0.3261 - accuracy: 0.8614 - val_loss: 0.3456 - val_accuracy: 0.8542\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 558s 893ms/step - loss: 0.3074 - accuracy: 0.8723 - val_loss: 0.3372 - val_accuracy: 0.8558\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 556s 890ms/step - loss: 0.2938 - accuracy: 0.8788 - val_loss: 0.3053 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 557s 891ms/step - loss: 0.2744 - accuracy: 0.8886 - val_loss: 0.2934 - val_accuracy: 0.8826\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 588s 942ms/step - loss: 0.2651 - accuracy: 0.8917 - val_loss: 0.2960 - val_accuracy: 0.8776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7c90efa64970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "GTedJiIJSy8P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTedJiIJSy8P",
    "outputId": "e2efcd2e-bcf1-47db-89a3-41bf7cddb9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 166s 207ms/step - loss: 0.2991 - accuracy: 0.8730\n",
      "Test acc: 0.873\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4TL4CAb-toz",
   "metadata": {
    "id": "f4TL4CAb-toz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
