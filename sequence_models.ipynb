{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418208bb-73f0-4b89-8d51-7bdc2f07d9f5",
   "metadata": {},
   "source": [
    "# Deep learning for text: sequence models\n",
    "With the improvement using the bigram data we have seen that the order of words matters. Now we want to study more in depth how to exploit the order of words in texts to perform several tasks that are common when using textual data. Instead of creating N-grams we will let the model figure out how to use a text to address the task at hand using a sequnce model. We start by assigning an integer index to each word in a vocabulary that will be mapped to a vector. These vectors represent a base vectors that will be used to represent every text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f9f448-a8a6-4b50-a005-ab55b692408c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37812cb-1e63-4f4e-8c9b-30c0f66bee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8046dea-633e-436c-af1a-f3a7d4625f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
