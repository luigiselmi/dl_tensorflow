{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFadtwaFcnyzZePzVrfhCa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luigiselmi/dl_tensorflow/blob/main/regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "The goal of a regression algorithm is to find out the relationship between two or more continuous variables. In this notebook we will try to compute the price of houses in Boston from a set of numerical features. The [Boston Housing](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) dataset is characterized by the small size of the dataset and by the different ranges of values of each feature. Also this dataset can be downloaded from Tensorflow. The features of the dataset are\n",
        "\n",
        "1. CRIM - per capita crime rate by town\n",
        "2. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS - proportion of non-retail business acres per town.\n",
        "4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
        "5. NOX - nitric oxides concentration (parts per 10 million)\n",
        "6. RM - average number of rooms per dwelling\n",
        "7. AGE - proportion of owner-occupied units built prior to 1940\n",
        "8. DIS - weighted distances to five Boston employment centres\n",
        "9. RAD - index of accessibility to radial highways\n",
        "10. TAX - full-value property-tax rate per 10000 dollars\n",
        "11. PTRATIO - pupil-teacher ratio by town\n",
        "12. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT - % lower status of the population\n",
        "14. MEDV - Median value of owner-occupied homes in $1000's\n",
        "\n",
        "The target feauture is MEDV, the median value of owner-occupied homes"
      ],
      "metadata": {
        "id": "zKvC9Q6oQS6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = (boston_housing.load_data())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJyHYeuAQSGE",
        "outputId": "3d53f702-c4a7-4f07-d02b-361543e0492c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY1hjITeQQ0a",
        "outputId": "61435854-1752-4c34-d2e3-499b3b910321"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (102, 13))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data normalization\n",
        "It might be difficult to model to deal with feautures with different ranges of values. A simple technique to avoid such problem is to standardize each feature by subtracting itm mean value and dividing by its standard deviation."
      ],
      "metadata": {
        "id": "GnsWG_nvcPa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "metadata": {
        "id": "oV89udfcSH-w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shYN7VskdnZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}